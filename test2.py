from pathlib import Path
import subprocess
from datetime import datetime
import numpy as np # type: ignore
import cv2 # type: ignore

def merge_bboxes(bboxes, iou_threshold=0.05):#iou_thresholdæ ãŒã©ã‚“ã ã‘ã‹ã¶ã£ã¦ã‚‹ã‹
    """
    è¤‡æ•°ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’çµ±åˆã™ã‚‹é–¢æ•°
    è¿‘æ¥ã—ã¦ã„ã‚‹ãƒœãƒƒã‚¯ã‚¹ã‚’çµ±åˆã—ã¾ã™ã€‚
    iou_threshold: çµ±åˆã™ã‚‹ãŸã‚ã®IoUé–¾å€¤
    """
    if len(bboxes) == 0:
        return []

    # IoUè¨ˆç®—é–¢æ•°
    def iou(bbox1, bbox2):
        x1, y1, w1, h1 = bbox1
        x2, y2, w2, h2 = bbox2
        inter_area = max(0, min(x1 + w1, x2 + w2) - max(x1, x2)) * max(0, min(y1 + h1, y2 + h2) - max(y1, y2))
        union_area = w1 * h1 + w2 * h2 - inter_area
        iou_value = inter_area / union_area

        # IoUã®å€¤ã‚’è¡¨ç¤ºã—ã¦ãƒ‡ãƒãƒƒã‚°ã™ã‚‹
        print(f"IoU between {bbox1} and {bbox2}: {iou_value}")
    
        return iou_value  

    # é‡ãªã£ã¦ã„ã‚‹ãƒœãƒƒã‚¯ã‚¹ã‚’çµ±åˆ
    merged_bboxes = []
    for bbox in bboxes:
        if not merged_bboxes:
            merged_bboxes.append(bbox)
            continue

        merged = False
        for i, merged_bbox in enumerate(merged_bboxes):
            if iou(bbox, merged_bbox) > iou_threshold:
                # çµ±åˆã™ã‚‹å ´åˆã€å¹³å‡ã‚’å–ã£ã¦ä¸€ã¤ã®æ ã«ã™ã‚‹
                merged_bboxes[i] = [
                    (bbox[0] + merged_bbox[0]) / 2,  # x_center
                    (bbox[1] + merged_bbox[1]) / 2,  # y_center
                    (bbox[2] + merged_bbox[2]) / 2,  # width
                    (bbox[3] + merged_bbox[3]) / 2   # height
                ]
                merged = True
                break

        if not merged:
            merged_bboxes.append(bbox)

    return merged_bboxes

def merge_audio_video(video_file, audio_file, output_file, ffmpeg_path):
    """
    æ˜ åƒã¨éŸ³å£°ã‚’çµ±åˆã™ã‚‹é–¢æ•°
    video_file: æ˜ åƒãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆmp4ï¼‰
    audio_file: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆm4aï¼‰
    output_file: å‡ºåŠ›å…ˆã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«
    ffmpeg_path: ffmpegã®ãƒ‘ã‚¹
    """
    print("ğŸ¬ æ˜ åƒã¨éŸ³å£°ã‚’çµåˆä¸­...")
    subprocess.run([
        str(ffmpeg_path),
        "-y",  # å‡ºåŠ›å…ˆãŒã‚ã‚Œã°ä¸Šæ›¸ã
        "-i", str(video_file),  # æ˜ åƒãƒ•ã‚¡ã‚¤ãƒ«
        "-i", str(audio_file),  # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«
        "-c:v", "copy",         # æ˜ åƒã¯å†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãªã„
        "-c:a", "aac",          # éŸ³å£°ã¯aacã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        "-strict", "experimental",  # AACã®ä½¿ç”¨
        str(output_file)        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«
    ])

    if not output_file.exists():
        print(f"âŒ ãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ã¾ã—ãŸ: {output_file}")
        exit()
    print("ğŸ¬ éŸ³å£°ã¨æ˜ åƒã®çµåˆå®Œäº†ï¼")

# === å…¥åŠ› ===
url = input("ğŸ¥ åˆ†æã—ãŸã„YouTubeã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼š\n")

# === ãƒ‘ã‚¹è¨­å®š ===
yt_dlp_path = Path("C:/stream_ai/yt-dlp.exe")
ffmpeg_path = Path("C:/stream_ai/ffmpeg-2025-03-31-git-35c091f4b7-essentials_build/bin/ffmpeg.exe")
model_path = Path("C:/stream_ai/runs/detect/train7/weights/best.pt")

# === å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ ===
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = Path(f"C:/stream_ai/outputs/{timestamp}")
output_dir.mkdir(parents=True, exist_ok=True)

downloaded_base = output_dir / "test_video"
short_video = output_dir / "test_video_short.mp4"

# === ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ===
print("ğŸ“¥ YouTubeå‹•ç”»ã‚’é«˜ç”»è³ªã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
subprocess.run([  # å‹•ç”»ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    str(yt_dlp_path),
    "-f", "bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4",
    "-o", str(downloaded_base) + ".%(ext)s",
    url
])

video_file = downloaded_base.with_suffix(".f399.mp4")
audio_file = downloaded_base.with_suffix(".f140.m4a")
merged_file = downloaded_base.with_suffix(".mp4")

# === ãƒãƒ¼ã‚¸å‡¦ç†ï¼ˆæ˜ åƒï¼‹éŸ³å£°ï¼‰ ===
merge_audio_video(video_file, audio_file, merged_file, ffmpeg_path)

# === 3åˆ†åˆ‡ã‚Šå‡ºã— ===
print("åˆ‡ã‚Šå‡ºã—ä¸­...")
subprocess.run([
    str(ffmpeg_path),
    "-y",
    "-i", str(merged_file),
    "-t", "10",
    "-c:v", "libx264",
    "-crf", "23",
    "-preset", "fast",
    "-c:a", "aac",
    str(short_video)
])

# === æ¨è«–ï¼ˆYOLOï¼‰ ===
print("ğŸ” YOLOv8ã§ã‚«ãƒ¡ãƒ©æ ã‚’æ¤œå‡ºä¸­...")
subprocess.run([
    "yolo",
    "task=detect",
    "mode=predict",
    f"model={model_path}",
    f"source={short_video}",
    "save=True",
    "save_crop=True",
    "imgsz=1280",
    "conf=0.2",
    f"project={output_dir}",
    "name=predict"
])

# === æ ã®çµ±åˆå‡¦ç† ===
detected_boxes = [...]  # YOLOv8ã®æ¤œå‡ºçµæœ
merged_boxes = merge_bboxes(detected_boxes)

# çµ±åˆã•ã‚ŒãŸæ ã‚’æ–°ã—ã„å‹•ç”»ã«æç”»ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ 
for box in merged_boxes:
    pass  # æ ã‚’æç”»ã™ã‚‹å‡¦ç†ï¼ˆä¾‹ãˆã°ã€OpenCVãªã©ã‚’ä½¿ã£ã¦



# YOLOå‡ºåŠ›çµæœï¼ˆç”»åƒï¼†ãƒ©ãƒ™ãƒ«ï¼‰ã‚’èª­ã¿è¾¼ã‚“ã§å‡¦ç†
predict_dir = output_dir / "predict"  # YOLOæ¨è«–å¾Œã®ç”»åƒï¼†txtãŒã‚ã‚‹å ´æ‰€
merged_dir = predict_dir / "merged_boxes"
merged_dir.mkdir(exist_ok=True)

for txt_file in predict_dir.glob("*.txt"):
    img_file = txt_file.with_suffix(".jpg")
    if not img_file.exists():
        continue

    # --- YOLOçµæœã®èª­ã¿è¾¼ã¿ ---
    bboxes = []
    with open(txt_file, "r") as f:
        for line in f:
            parts = line.strip().split()
            if len(parts) >= 5:
                x, y, w, h = map(float, parts[1:5])
                bboxes.append([x, y, w, h])

    # --- æ ã‚’çµ±åˆ ---
    merged = merge_bboxes(bboxes, iou_threshold=0.2)

    # --- ç”»åƒèª­ã¿è¾¼ã¿ & æç”» ---
    img = cv2.imread(str(img_file))
    h_img, w_img = img.shape[:2]

    for box in merged:
        cx, cy, bw, bh = box
        x1 = int((cx - bw / 2) * w_img)
        y1 = int((cy - bh / 2) * h_img)
        x2 = int((cx + bw / 2) * w_img)
        y2 = int((cy + bh / 2) * h_img)
        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)

    # --- ä¿å­˜ ---
    out_path = merged_dir / img_file.name
    cv2.imwrite(str(out_path), img)

print("âœ… çµ±åˆã•ã‚ŒãŸæ ã‚’æç”»ã—ã¦ä¿å­˜ã—ã¾ã—ãŸï¼")
print(f"ğŸ“ å‡ºåŠ›å…ˆ: {merged_dir}")

print(f"\nâœ… åˆ†æå®Œäº†ï¼\nğŸ“ ä¿å­˜å…ˆ: {output_dir}")
